{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "metadata": {
        "id": "fcDuBm2PtSrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "-JWWlnbMt3b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Путь к датасету CLEVR\n",
        "clevr_dir = 'gdrive/path_to_clevr_dataset'  # Указываем путь к папке с данными CLEVR\n",
        "\n",
        "# Загрузка данных (вопросы и ответы)\n",
        "with open(os.path.join(clevr_dir, 'questions/CLEVR_train_questions.json')) as f:\n",
        "    train_questions = json.load(f)  # Загружаем JSON с вопросами и ответами для тренировочного набора"
      ],
      "metadata": {
        "id": "qIvCMlwqtqZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Предобработка вопросов и ответов\n",
        "questions = [q['question'] for q in train_questions['questions']]  # Извлекаем текст вопросов\n",
        "answers = [q['answer'] for q in train_questions['questions']]      # Извлекаем текст ответов\n",
        "\n",
        "# Преобразование текстовых вопросов в числовой формат\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=5000)  # Создаем токенайзер с ограничением в 5000 слов\n",
        "tokenizer.fit_on_texts(questions)                               # Обучаем токенайзер на тексте вопросов\n",
        "\n",
        "questions_seq = tokenizer.texts_to_sequences(questions)         # Преобразуем вопросы в последовательности чисел (индексы слов)\n",
        "questions_padded = keras.preprocessing.sequence.pad_sequences(questions_seq, maxlen=30)  # Приводим последовательности к одинаковой длине (30)\n",
        "\n"
      ],
      "metadata": {
        "id": "TdUfskYntYZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка изображений сцены\n",
        "image_dataset = image_dataset_from_directory(\n",
        "    os.path.join(clevr_dir, 'images/train'),   # Путь к папке с изображениями тренировочного набора\n",
        "    image_size=(128, 128),                     # Приводим все изображения к размеру 128x128\n",
        "    batch_size=32                              # Устанавливаем размер батча\n",
        ")"
      ],
      "metadata": {
        "id": "JCN7qRJdttjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Определение модели\n",
        "# CNN для извлечения признаков из изображения\n",
        "def build_cnn_model():\n",
        "    cnn_base = tf.keras.applications.ResNet50(           # Используем предобученную модель ResNet50 в качестве основы для CNN\n",
        "        include_top=False,                               # Исключаем последние слои для классификации\n",
        "        input_shape=(128, 128, 3),                       # Устанавливаем размер входного изображения\n",
        "        pooling='avg'                                    # Применяем Global Average Pooling для получения признаков\n",
        "    )\n",
        "    cnn_base.trainable = False  # Замораживаем веса модели, чтобы они не обучались на этапе тренировки\n",
        "    return cnn_base             # Возвращаем базовую модель CNN"
      ],
      "metadata": {
        "id": "5msQKOJeuVp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Трансформер для обработки вопросов\n",
        "class TransformerBlock(layers.Layer):  # Создаем пользовательский слой для трансформерного блока\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)  # Мультиголовное внимание\n",
        "        self.ffn = keras.Sequential([                                                  # FFN (Feed Forward Network) для нелинейных преобразований\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),                                   # Первый слой с активацией ReLU\n",
        "            layers.Dense(embed_dim)                                                    # Второй слой для восстановления исходной размерности\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)  # Нормализация слоя для стабилизации\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)  # Еще одна нормализация слоя\n",
        "        self.dropout1 = layers.Dropout(rate)                       # Dropout для регуляризации\n",
        "        self.dropout2 = layers.Dropout(rate)                       # Dropout после FFN\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)  # Операция внимания: каждая часть текста взаимодействует с каждой другой\n",
        "        attn_output = self.dropout1(attn_output, training=training)  # Применяем Dropout в режиме обучения\n",
        "        out1 = self.layernorm1(inputs + attn_output)  # Добавляем резидуальное соединение и нормализуем результат\n",
        "        ffn_output = self.ffn(out1)  # Пропускаем через FFN (Feed Forward Network)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)  # Применяем Dropout во второй раз\n",
        "        return self.layernorm2(out1 + ffn_output)  # Добавляем резидуальное соединение и нормализуем снова\n",
        "\n",
        "# Вспомогательный класс для добавления позиционной информации (необходим для трансформера)\n",
        "class PositionalEncoding(layers.Layer):  # Создаем слой для добавления позиционных кодировок\n",
        "    def __init__(self, max_len, embed_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(max_len, embed_dim)  # Генерируем позиционные кодировки\n",
        "\n",
        "    def get_angles(self, pos, i, embed_dim):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(embed_dim))  # Определяем шкалу для позиционного кодирования\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def positional_encoding(self, position, embed_dim):\n",
        "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
        "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # Применяем синус к четным индексам\n",
        "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # Применяем косинус к нечетным индексам\n",
        "        pos_encoding = angle_rads[np.newaxis, ...]         # Добавляем размерность для батча\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)     # Преобразуем к типу float32\n",
        "\n",
        "    def call(self, x):\n",
        "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]  # Добавляем позиционное кодирование к входным данным"
      ],
      "metadata": {
        "id": "fhVDk1otufsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Полная модель\n",
        "def build_vqa_model(vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes):\n",
        "    # Вход для изображения\n",
        "    image_input = layers.Input(shape=(128, 128, 3))     # Входной слой для изображений (размер 128x128x3)\n",
        "    cnn_model = build_cnn_model()                       # Используем нашу модель CNN для извлечения признаков из изображений\n",
        "    image_features = cnn_model(image_input)             # Извлекаем признаки изображения\n",
        "\n",
        "    # Вход для вопроса\n",
        "    question_input = layers.Input(shape=(30,))          # Входной слой для вопросов (длина последовательности 30)\n",
        "    embedding = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(question_input)  # Преобразуем слова в вектора (Embedding)\n",
        "\n",
        "    # Добавляем позиционное кодирование к векторным представлениям вопросов\n",
        "    x = PositionalEncoding(30, embed_dim)(embedding)\n",
        "\n",
        "    # Пропускаем через несколько трансформерных блоков\n",
        "    for _ in range(num_blocks):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    # Объединение признаков изображения и вопроса\n",
        "    combined = layers.concatenate([image_features, layers.Flatten()(x)])  # Объединяем признаки изображения и вопросы в один вектор\n",
        "\n",
        "    # Финальный слой классификации для предсказания ответа\n",
        "    output = layers.Dense(num_classes, activation='softmax')(combined)  # Применяем softmax для классификации ответов\n",
        "\n",
        "    # Создание модели\n",
        "    model = keras.Model(inputs=[image_input, question_input], outputs=output)  # Определяем модель с двумя входами (изображение и вопрос)\n",
        "    return model"
      ],
      "metadata": {
        "id": "VgML_fyTujQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Компиляция и обучение модели\n",
        "# Получение количества уникальных ответов для задания количества классов\n",
        "answer_tokenizer = keras.preprocessing.text.Tokenizer()  # Создаем токенайзер для ответов\n",
        "answer_tokenizer.fit_on_texts(answers)                   # Обучаем токенайзер на ответах\n",
        "answers_seq = answer_tokenizer.texts_to_sequences(answers)  # Преобразуем ответы в числовые последовательности\n",
        "answers_categorical = keras.utils.to_categorical(answers_seq, num_classes=len(answer_tokenizer.word_index) + 1)  # Преобразуем в one-hot encoding\n",
        "\n",
        "# Построение модели VQA\n",
        "model = build_vqa_model(vocab_size=5000, embed_dim=64, num_heads=8, ff_dim=128, num_blocks=4, num_classes=len(answer_tokenizer.word_index) + 1)\n",
        "\n",
        "# Компиляция модели\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Компилируем модель с Adam и функцией потерь для классификации\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(\n",
        "    [image_dataset, questions_padded],   # Передаем изображения и закодированные вопросы в модель\n",
        "    answers_categorical,                 # Ответы в формате one-hot\n",
        "    epochs=10,                           # Количество эпох обучения\n",
        "    batch_size=32                        # Размер батча\n",
        ")\n"
      ],
      "metadata": {
        "id": "1XRMeE18tNBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lGlerRwGMrZX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}